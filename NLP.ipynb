{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_PR_9nNUjbrC",
        "NCZmQMVTqIfh",
        "6PG-iFopfBTe",
        "fs1wvAj8fKv-",
        "S-a2-7mVn_07",
        "brd2T0v3zE8H",
        "0suZz5QW0x06",
        "h_J087H05cfx",
        "Ph_poUOQglBg",
        "Yyx951TJ40VJ",
        "_tcsfRgExMOf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JefteLG/Estudos/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JovPxtaLixHF"
      },
      "source": [
        "<h1><strong><center>Processamento de Linguagem Natural (Natural Language Processing)<center></strong><h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzEgL-FrjFXd"
      },
      "source": [
        "# <strong>Estudo(Tokenização Com TensorFlow e Keras)</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG2TTgyqcsql"
      },
      "source": [
        "**<h2>`OBS.:`<h2>**\n",
        "- **<h2>`Ao tokenizar poucas ou diversas palavras, as que mais repete recebe o menor token numerico.`<h2>**\n",
        "\n",
        "- **<h2>`A palavra que recebe o token numerico é convertida para lowercase, ou seja, toda palavra fica em minusculo.`<h2>**\n",
        "\n",
        "- **<h2>`0 é um índice reservado que não será atribuído a nenhuma palavra.`<h2>**\n",
        "\n",
        "- **<h2>Esse notebook foi criado com o intuito de explicar mais detalhadamente e simplicar alguns conceitos e partes importantes abordadas de maneira rapidas nos videos presente na playlist <a href=\"https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S&index=1&ab_channel=TensorFlow\">Natural Language Processing (NLP) Zero to Hero</a>.<h2>**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PR_9nNUjbrC"
      },
      "source": [
        "## <strong>Seção 01</strong>\n",
        "\n",
        "Como representar palavras de forma que um computador possa processa-las com o objetivo de treinar uma rede neural que possa entender o seu significado. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlzNbt_sZx-"
      },
      "source": [
        "- Para que o computador consiga reconhecer a frase é necessario realizar a codificação das palavras.\n",
        "\n",
        "- Nesse caso cada palavra vai ter um valor \n",
        "\n",
        "- Ex1.: `'I love my dog'` == `{'i': 1, 'love': 2, 'my': 3, 'dog': 4}`\n",
        "\n",
        "- Ex2.: `'I love my cat'` == `{'i': 1, 'love': 2, 'my': 3, 'cat': 5}`\n",
        "\n",
        "- Como as palavras `\"i, love, my\"` ja estão codificadas elas continuam com mesmo valor, já a palavra `\"cat\"` recebe um novo valor de codificação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNlEgJhnT0g"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Tokenizer é uma classe permite vetorizar um corpus de texto, transformando cada texto em uma sequência\n",
        "de inteiros (cada inteiro sendo o índice de um token em um dicionário) ou em um vetor onde o coeficiente\n",
        "de cada token pode ser binário, com base na contagem de palavras.\n",
        "\n",
        "Por padrão, toda pontuação é removida, transformando os textos em sequências de palavras separadas por\n",
        "espaço (as palavras podem incluir o caractere `'`). Essas sequências são então dividido em listas de\n",
        "tokens. Eles serão então indexados ou vetorizados.\n",
        "\"\"\"\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0mRTyl3n0Pp"
      },
      "source": [
        "#Array de frases para facilitar a codificação das palavras\n",
        "sentences = [\n",
        "  'I love my dog',\n",
        "  'I love my cat',\n",
        "  'I LOVE YOU DOG'\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCBHcUW5oE73"
      },
      "source": [
        "#Instanciar a classe Tokenizer e criar um objeto TK\n",
        "\"\"\"\n",
        "O argumento num_word define o número máximo de palavras a serem mantidas, com base na\n",
        "frequência das palavras. Apenas as palavras `num_words-1` mais comuns serão mantidas. \n",
        "\n",
        "Nesse caso, num_words=100, apenas as 100 palavras mais repitidas no array de frases serão codificadas. \n",
        "\"\"\"\n",
        "TK = Tokenizer(num_words=100)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dTrrCd5oZl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10376ff8-7bbf-4cce-afca-8b1853f6c8c2"
      },
      "source": [
        "\"\"\"\n",
        "O metodo fit_on_texts percorre todas as frases do array e então se ajusta a elas. Ou seja,\n",
        "esse metodo Atualiza o vocabulário interno com base em uma lista de textos.\n",
        "\n",
        "No caso em que os textos contêm listas, assumimos que cada entrada das listas é um token.\n",
        "\n",
        "A variavel word_index armazena os dados, sendo a chave a palavra e o valor o Token numerico,\n",
        "respectivamente, em um dicionario. Por meio do atributo word_index, acessado pelo objeto\n",
        "TK que foi instanciado pela classe Tokenizer.\n",
        "\n",
        "Ex1.: 'I love my dog' == {'i': 1, 'love': 2, 'my': 3, 'dog': 4}\n",
        "\n",
        "\"\"\"\n",
        "TK.fit_on_texts(sentences)\n",
        "word_index = TK.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCZmQMVTqIfh"
      },
      "source": [
        "##<strong>Seção 02</strong>\n",
        "Criar sequencias de numeros a partir de suas frases e usar ferramentas para processá-los e torná-los prontos para o ensino de redes neurais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PG-iFopfBTe"
      },
      "source": [
        "### <strong> 2.1 Transformar as palvras da frase em sequencias de tokens numericos </strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN7qZD4MVDlm"
      },
      "source": [
        "- IMPORTANTE: `Para inserir os dados na rede neural é necessario um pré-processamento dos dados.`\n",
        "\n",
        "- Até o momento foi codificando varias sentenças em tokens númericos.\n",
        "\n",
        "- Para inserir os dados em uma rede neural é necessario sequenciar os tokens numericos de cada array em tamanhos iguais de tokens por array.\n",
        "\n",
        "- Ex1.:\n",
        " - `'I love my dog'` == `{'i': 5, 'love': 3, 'my': 1, 'dog': 2}`\n",
        "\n",
        "- Ex2.:\n",
        " - `'You love my dog'` == `{'you': 4, 'love': 3, 'my': 1, 'dog': 2}`\n",
        "\n",
        "- Ex3.:\n",
        " - `'Do you think my dog is amazing?'` == `{'do': 6, 'you': 4, 'think': 7, 'my': 1, 'dog': 2, 'is': 8, 'amazing': 9}`\n",
        "\n",
        "- Os exemplos 1, 2 e 3 respectivamente resutariam na seguinte sequencia:\n",
        " - `Sequencias: [[5, 3, 1, 2], [4, 3, 1, 2], [6, 4, 7, 1, 2, 8, 9]]`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZXGBG4QhO5T"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA4TBynzhliZ"
      },
      "source": [
        "sentences = [\n",
        "  'I love my dog',\n",
        "  'I love my cat',\n",
        "  'You love my dog',\n",
        "  'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXhWh6uoqgTp"
      },
      "source": [
        "TK = Tokenizer(num_words=100)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzt9JtoBqy89"
      },
      "source": [
        "TK.fit_on_texts(sentences)\n",
        "word_index = TK.word_index\n",
        "# texts_to_sequences é utilizado para criar uma sequencia de tokens de acordo com o corpus\n",
        "sequences = TK.texts_to_sequences(sentences)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C4Emx-3cpSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad6c5f2-9218-4298-bef2-d68c3de7418e"
      },
      "source": [
        "print(f'indice das palavras(Tokens): {word_index}') \n",
        "print(f'Sequencias: {sequences}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "indice das palavras(Tokens): {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "Sequencias: [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs1wvAj8fKv-"
      },
      "source": [
        "### <strong>2.2 lidando com palavras não conhecida(sem codificação)</strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzGX8qXMhRHT"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "  'I love my dog',\n",
        "  'I love my cat',\n",
        "  'You love my dog',\n",
        "  'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "TK.fit_on_texts(sentences)\n",
        "word_index = TK.word_index"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNHPhPGqfYo7"
      },
      "source": [
        "- Ao classificar as palavra a rede neural pode-se encontrar uma ou mais palavras que não possui codificação, logo, a palavra não é reconhecida, acarretando confusões no Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL0FOf_RjNv0"
      },
      "source": [
        "- O script abaixo executa um metodo pertecente ao objeto TK instanciado da classe Tokenizer, que é responsavel pelo sequenciamento dos tokens das palavras, entretanto, a sequencia de tokens faltou algumas palavras, devido ao utilizar palavras que o Tokenizer ainda não tinha codificado, logo, não existe no indice de palavras(word_index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN94-BOlhbCX",
        "outputId": "9a5e06e4-744d-4741-99fe-60749ff59ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"\n",
        "Dados para testar o metodo text_to_sequences e ver as palavras que não possui indeces, são elas: (\"really, loves, manatee\").\n",
        "Isso ocorreu porque essas palavras não estão no meu conjunto de dados iniciais onde foi executato o metodo fit_on_texts, no qual é\n",
        "responsavel pela criação dos Tokens numericos .\n",
        "\"\"\"\n",
        "data_test = [\n",
        "  'I really love my dog',\n",
        "  'My dog loves my manatee'\n",
        "]\n",
        "\n",
        "test_seq = TK.texts_to_sequences(data_test)\n",
        "print(f'sequencias de teste: {test_seq}')\n",
        "print(f'indice das palavras{word_index}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequencias de teste: [[4, 2, 1, 3], [1, 3, 1]]\n",
            "indice das palavras{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m44ngBDLlPYA"
      },
      "source": [
        "- É importante sempre ter uma quantidade boa(grande) de indece de palavras(word_index), para lidar com palavras e/ou frases que não estão no conjunto de treinamento "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-a2-7mVn_07"
      },
      "source": [
        "### <strong>2.3 Usando o argumento oov_token para não perder a duração da sequencia</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7YnVLJGp0OV"
      },
      "source": [
        "- Para prezervar o tamanho da sequencia mesmo se uma ou mais palavras não sendo reconhecida, é utilizado o argumento oov_token.\n",
        "\n",
        "- Ao usar o metodo fit_on_texts, se fornecido o argumento oov_token, será adicionado a word_index e usado para substituir palavras fora do vocabulário durante as chamadas text_to_sequence.\n",
        "\n",
        "- Simplificando: Com esse argumento o Tokenizer reserva um token para atribuir a palavras não identificadas no `(vocabulario/indice de palavras/word_index) = tudo a mesma coisa`.\n",
        "\n",
        "- `Muito mais Simplificado: Caso alguma palavras não seja identificada a mesma sera substituida por outra qualquer especificada.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvV64-oGrByc",
        "outputId": "059209b7-581f-4927-cb91-f79e9fdc9e00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "  'I love my dog',\n",
        "  'I love my cat',\n",
        "  'You love my dog',\n",
        "  'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "TK = Tokenizer(num_words=100, oov_token='<oov>')\n",
        "\n",
        "TK.fit_on_texts(sentences)\n",
        "word_index = TK.word_index\n",
        "\n",
        "sequences = TK.texts_to_sequences(sentences)\n",
        "\n",
        "data_test = [\n",
        "  'I really love my dog',\n",
        "  'My dog loves my manatee'\n",
        "]\n",
        "\n",
        "test_seq = TK.texts_to_sequences(data_test)\n",
        "\n",
        "print(f'word_index : {word_index}')\n",
        "print(f'Sequencias_TREINO: {sequences}')\n",
        "print(f'Sequencias_TESTE: {test_seq}')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_index : {'<oov>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "Sequencias_TREINO: [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "Sequencias_TESTE: [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brd2T0v3zE8H"
      },
      "source": [
        "### <strong>2.4 Como a rede neural lida com frases de tamanho diferentes?</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bbxogHhzgYO"
      },
      "source": [
        "- Existe um metodo avançado chamado RaggedTensor que torna mais fácil armazenar e processar dados com formas não uniformes. Entretanto, eu busco algo mais simples.\n",
        "\n",
        "- Para resolver o problema de tamanhos variaveis é usado a solução de preenchimento, com a função pad_sequences.\n",
        "\n",
        "- Esta função transforma uma lista (de comprimento `num_samples`) de sequências (listas de inteiros) em uma matriz Numpy 2D de forma `(num_samples, num_timesteps)`. `num_timesteps` é o argumento` maxlen` se fornecido, ou o comprimento da sequência mais longa da lista.\n",
        "\n",
        "- Sequências mais curtas que `num_timesteps` são preenchidos com `valor` até que tenham` num_timesteps`.\n",
        "\n",
        "- Sequências maiores que `num_timesteps` são truncadas para que se ajustem ao comprimento desejado.\n",
        "\n",
        "- A posição onde o preenchimento ou truncamento acontece é determinada por os argumentos `padding` e` truncating`, respectivamente. Pré-preenchimento ou remoção de valores do início da sequência é o padrão.\n",
        "\n",
        "- <strong>Simplificando: A função pad_sequences transforma uma lista de sequências (listas de inteiros) em uma matriz Numpy 2D, preencheendo as sequencias de tamanho variado, resultanto em sequencias de tamanhos iguais.</strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH2BlNHA1VSz"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#Importação da Função responsavel pelo preenchimento uniforme das sequencias \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec-bSVe2l47p"
      },
      "source": [
        "sentences = [\n",
        "  'I love my dog',\n",
        "  'I love my cat',\n",
        "  'You love my dog',\n",
        "  'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLFPTSWvmB8X"
      },
      "source": [
        "TK = Tokenizer(num_words=100, oov_token='oov')\n",
        "TK.fit_on_texts(sentences)\n",
        "word_index = TK.word_index"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByiUWUIamhjH"
      },
      "source": [
        "sequences = TK.texts_to_sequences(sentences)\n",
        "\"\"\"\n",
        "Se o argumento argumento maxlen não for fornecido, o preenchimento vai ter\n",
        "como base o comprimento da sequência mais longa da lista.\n",
        "\n",
        "Com o maxlen definido o comprimento de uma sequencia é referente ao valor do\n",
        "maxlen.\n",
        "\n",
        "Se a sequencia for maior que o maxlen especificado os dados serão\n",
        "perdidos/descatados, caso queira um truncating especifico use truncating='post'\n",
        "para descartar os dados à direita, ou somente truncating para descartar os dados\n",
        "à esquerda.\n",
        "\n",
        "Caso queira os zeros à direita é só usar o parametro padding com o valor\n",
        "'post'(padding = 'post').\n",
        "\"\"\"\n",
        "# A variavel padded recebe uma matriz Numpy 2D de forma (num_samples, num_timesteps).\n",
        "# num_timesteps é o argumento maxlen se fornecido, ou o comprimento da sequência mais longa da lista.\n",
        "padded = pad_sequences(sequences, padding = 'post', maxlen=5, truncating='post')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJhKYNLgm1mF",
        "outputId": "74957376-b0e8-4cc6-d01a-85c66e0f4531",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'word_index: {word_index}')\n",
        "print(f'Sequences: {sequences}')\n",
        "print(f'{padded}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_index: {'oov': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "Sequences: [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "[[5 3 2 4 0]\n",
            " [5 3 2 7 0]\n",
            " [6 3 2 4 0]\n",
            " [8 6 9 2 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0suZz5QW0x06"
      },
      "source": [
        "## <strong>Aplicação_01</strong>\n",
        "<h1><strong>Construindo um classificador capaz de identificar sentimentos no texto.</strong><h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5tI15AM7Ev1"
      },
      "source": [
        "# Variaveis essenciais no decorrer da execução do script\n",
        "# Já executa logo\n",
        "\n",
        "# apenas as 10000 palavras mais repitidas no array de frases serão codificadas.\n",
        "vocab_size = 10000\n",
        "\n",
        "# Dimensão da incorporação densa.\n",
        "embedding_dim = 16\n",
        "\n",
        "# Comprimento das sequências de entrada, quando é constante\n",
        "max_length = 100\n",
        "\n",
        "# Quebrar a direita da sequencia\n",
        "trunc_type='post'\n",
        "\n",
        "# Preencher a direita\n",
        "padding_type='post'\n",
        "\n",
        "# valor para substituir os valores não reconhecido\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# tamanho dos dados de treinamento\n",
        "training_size = 20000\n",
        "\n",
        "# Quantidade de vezes que o processamento pelo algoritmo de aprendizagem vai ser repetido em todo o conjunto de treinamento.\n",
        "num_epochs = 30"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_J087H05cfx"
      },
      "source": [
        "### **Tratar e selecionar os dados Necessarios**\n",
        "Converter os objetos JSON para python e selecionar apenas os dados necessario para o Pré-Processamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn3Bcmix3ikU"
      },
      "source": [
        "- Vamos treinar um classificador capaz de indentificar trechos de textos com caracteristicas sacastica, usando um dataset com diversos trechos sacaticos e não sacasticos.\n",
        "\n",
        "- Os dados estão armazenados em formato JSON, para treinar o modelo de rede neural é preciso converte-lo para o formato python.\n",
        "\n",
        "- Ao converter para python cada elemento do JSON vai se emcapsulado. Para fazer isso vamos utilizar o pacote json.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNmUsAjm1gUV"
      },
      "source": [
        "import json"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZqyw32l6IMC"
      },
      "source": [
        "OBS.: A celula abaixo é opcional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAKAMf-X6JG8"
      },
      "source": [
        "# Esse script pode ser usado caso você tenha o base de dados no computador e queira carrega-la no google colab. \n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM4fR3S0lsGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78319876-53fc-4f57-a3c0-ef57b4ba5027"
      },
      "source": [
        "# A base de dados desse exemplo está no meu repositorio do github e pode ser baixada no link abaixo\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/JefteLG/datasets/main/Sarcasm_Headlines_Dataset.json \\\n",
        "    -O /tmp/sarcasm.json"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-30 15:13:16--  https://raw.githubusercontent.com/JefteLG/datasets/main/Sarcasm_Headlines_Dataset.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5616830 (5.4M) [text/plain]\n",
            "Saving to: ‘/tmp/sarcasm.json’\n",
            "\n",
            "/tmp/sarcasm.json   100%[===================>]   5.36M  28.3MB/s    in 0.2s    \n",
            "\n",
            "2020-12-30 15:13:16 (28.3 MB/s) - ‘/tmp/sarcasm.json’ saved [5616830/5616830]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bI7Eepo1-Rq"
      },
      "source": [
        "O codigo abaixo vai carregar os dados em json e vai armazena-lo em uma variavel nomeada como `data`. Isso é necessario porque os dados estão em formato JSON e para utilizar no treinamento e teste é preciso converter para o formato python, nesse caso, vai ser uma lista de objetos JSON."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M4dNaho3S2d"
      },
      "source": [
        "**OBS.: LEIA OS COMENTARIOS COM ATENÇÃO.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5BYT_dvjrbe"
      },
      "source": [
        "# Pegar cada linha do arquivo JSON(base de dados) e armazenar em uma lista\n",
        "\n",
        "data = []\n",
        "\n",
        "for line in open('/tmp/sarcasm.json', 'r'):\n",
        "  data.append(json.loads(line))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFnKkwtQp1Z3"
      },
      "source": [
        "# Não é necessario executar esse codigo, ele é usado apenas para melhorar o arquivo JSON inicial\n",
        "# Serializar\n",
        "\n",
        "# with open('/tmp/sarcasm.json', 'w') as json_file:\n",
        "#     json.dump(data, json_file, indent=4)\n",
        "\n",
        "# with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
        "#   datastore = json.load(f)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNWkK0lyyI2B"
      },
      "source": [
        "# Percorrer a lista para ter acesso as informações necessaria e armazena-las e locais diferentes\n",
        "\n",
        "labels=[]\n",
        "sentences=[]\n",
        "\n",
        "for dt in data:\n",
        "  sentences.append(dt['headline'])\n",
        "  labels.append(dt['is_sarcastic'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph_poUOQglBg"
      },
      "source": [
        "### **Divisão dos dados**\n",
        "Divisão dos dados de treinamento para criar o modelo e dados de teste para medir a eficiencia do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paPcVdGQhVjo"
      },
      "source": [
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyx951TJ40VJ"
      },
      "source": [
        "### **Pré-Processamento dos dados utilizados para criação do modelo**\n",
        "Tokenizar os dados, preencher as sequencias, e padronizar o tamanho das sequancias dos dados selecionados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC50w5CX1oTh"
      },
      "source": [
        "# classe permite vetorizar um corpus de texto, transformando cada texto em uma sequência de inteiros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Importação da Função responsavel pelo preenchimento uniforme das sequencias\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foaCmT1Z6ugw"
      },
      "source": [
        "#Instanciar a classe Tokenizer e criar um objeto TK\n",
        "TK = Tokenizer(num_words=vocab_size, oov_token=oov_tok)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfCJ91Kh7f4x"
      },
      "source": [
        "# O metodo fit_on_texts é utilizado para percorrer todas as frases do array e então se ajusta a elas.\n",
        "TK.fit_on_texts(training_sentences)\n",
        "\n",
        "# A variavel word_index armazena os dados em um dicionario, onde, a chave é a palavra, e o valor é o Token numerico, respectivamente.\n",
        "# A variavel word_index recebe os valores referente ao atributo word_index, acessado pelo objeto TK que foi instanciado pela classe Tokenizer.\n",
        "word_index = TK.word_index"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PckCZ2T8stNn"
      },
      "source": [
        "- No momento de tokennizar a base de dados é interessante separar os dados de treino e teste para verificar a eficiencia do modelo.\n",
        "\n",
        "- Fazendo isso vamos testar com mais precisão o quão bom está o modelo, simulando palavras e frases que possivelmente não foram codificada, e como o modelo vai reagir nesse cenario. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBgU6Cxo8W0R"
      },
      "source": [
        "# A variavel training_sequences recebe o valor retornado pelo metodo texts_to_sequences, no qual, é utilizado para criar uma sequencia de tokens de acordo com o corpus.\n",
        "# Gera sequancias de tokens\n",
        "training_sequences = TK.texts_to_sequences(training_sentences)\n",
        "\n",
        "# A variavel padded recebe uma matriz Numpy 2D de forma (num_samples, num_timesteps).\n",
        "# num_timesteps é o argumento maxlen, o tamanho maximo se fornecido, ou o comprimento da sequência mais longa da lista.\n",
        "# Gera uma matriz de tokens padronizada para ser usada para treinar o modelo \n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "#dados de testes\n",
        "testing_sequences = TK.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwkM4kq-KOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a60811b-a294-4890-e939-9b1ecfb13fa4"
      },
      "source": [
        "# A primeira frase da base de dados referente aos dados de treino e teste, respectivamente, codificada e sequenciada por tokens.\n",
        "print(training_sequences[0])\n",
        "print(testing_sequences[0])\n",
        "\n",
        "# A primeira frase da base de dados referente aos dados de treino e teste, respectivamente, não codificada.\n",
        "print(training_sentences[0])\n",
        "print(testing_sentences[0])\n",
        "\n",
        "# A primeira frase da base de dados referente aos dados de treino e teste, respectivamente, codificada, sequenciada por tokens e com o tamanho padronizado.\n",
        "print(training_padded[0])\n",
        "print(testing_padded[0])\n",
        "\n",
        "# quantidade de sequancias e tokens por sequencias(dados de treino e teste, respectivamente).\n",
        "print(training_padded.shape)\n",
        "print(testing_padded.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[328, 1, 799, 3405, 2404, 47, 389, 2214, 1, 6, 2614, 8863]\n",
            "[1, 1100, 6663, 9423, 30, 1, 2439, 5, 519, 109]\n",
            "former versace store clerk sues over secret 'black code' for minority shoppers\n",
            "pediatricians announce 2011 newborns are ugliest babies in 30 years\n",
            "[ 328    1  799 3405 2404   47  389 2214    1    6 2614 8863    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   1 1100 6663 9423   30    1 2439    5  519  109    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "(20000, 100)\n",
            "(6709, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzBi7Dc6Q5rd"
      },
      "source": [
        "# arrays numpy multidimencionanis\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tcsfRgExMOf"
      },
      "source": [
        "### **Configurando a rede neural**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdeDKmxaEWQJ"
      },
      "source": [
        "- Vamos usar um modelo Sequencial para criar o nosso modelo de classificação.\n",
        "\n",
        "- Um modelo Sequential é apropriado para uma pilha simples de camadas onde cada camada tem exatamente um tensor de entrada e um tensor de saída. [**ESTUDAR**](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\n",
        "\n",
        "- Um modelo sequencial não é apropriado quando:\n",
        "  - Seu modelo possui múltiplas entradas ou múltiplas saídas\n",
        "  - Qualquer uma de suas camadas tem múltiplas entradas ou múltiplas saídas\n",
        "  - Você precisa fazer o compartilhamento de camadas\n",
        "  - Você deseja uma topologia não linear (por exemplo, uma conexão residual, um modelo de vários ramos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHmzCQLxUdD"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# A classe `Sequential` fornece recursos de treinamento e inferência neste modelo.\n",
        "# O modelo sequencial definido tem 4 camadas\n",
        "# Modelo sequencial criado passa uma lista de camadas para o construtor sequencial:\n",
        "model = tf.keras.Sequential([\n",
        "  # Transforma inteiros positivos (índices) em vetores densos de tamanho fixo.\n",
        "  # Esta camada só pode ser usada como a primeira camada em um modelo. Por ser responsavel pelo aprendizado das palavras, epoca por epoca\n",
        "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "  # Operação de agrupamento médio global para dados temporais, ou seja, vai somar todos os vetores e retorna um resultado, classificando o dando.\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  # Essas são duas camadas NN normal densamente conectada.\n",
        "  tf.keras.layers.Dense(24, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Configuração do modelo de treinamento\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR2tPS2LYfIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f140158-b162-465a-9bd0-4a4bbff9bb84"
      },
      "source": [
        "# visão geral do modelo\n",
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                408       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 160,433\n",
            "Trainable params: 160,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEw6kVLSFLQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d81b4f4-7ef9-42f0-dcf4-c62e9bc3b8a2"
      },
      "source": [
        "# Treina o modelo para um número fixo de épocas (iterações em um conjunto de dados).\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "625/625 - 3s - loss: 0.6608 - accuracy: 0.5891 - val_loss: 0.5820 - val_accuracy: 0.6698\n",
            "Epoch 2/30\n",
            "625/625 - 2s - loss: 0.4461 - accuracy: 0.8288 - val_loss: 0.4045 - val_accuracy: 0.8264\n",
            "Epoch 3/30\n",
            "625/625 - 2s - loss: 0.3241 - accuracy: 0.8727 - val_loss: 0.3604 - val_accuracy: 0.8487\n",
            "Epoch 4/30\n",
            "625/625 - 2s - loss: 0.2690 - accuracy: 0.8972 - val_loss: 0.3464 - val_accuracy: 0.8562\n",
            "Epoch 5/30\n",
            "625/625 - 2s - loss: 0.2327 - accuracy: 0.9107 - val_loss: 0.3435 - val_accuracy: 0.8526\n",
            "Epoch 6/30\n",
            "625/625 - 2s - loss: 0.2040 - accuracy: 0.9222 - val_loss: 0.3608 - val_accuracy: 0.8478\n",
            "Epoch 7/30\n",
            "625/625 - 2s - loss: 0.1822 - accuracy: 0.9327 - val_loss: 0.3681 - val_accuracy: 0.8483\n",
            "Epoch 8/30\n",
            "625/625 - 2s - loss: 0.1645 - accuracy: 0.9390 - val_loss: 0.3679 - val_accuracy: 0.8541\n",
            "Epoch 9/30\n",
            "625/625 - 2s - loss: 0.1474 - accuracy: 0.9481 - val_loss: 0.3943 - val_accuracy: 0.8475\n",
            "Epoch 10/30\n",
            "625/625 - 2s - loss: 0.1358 - accuracy: 0.9512 - val_loss: 0.4092 - val_accuracy: 0.8457\n",
            "Epoch 11/30\n",
            "625/625 - 2s - loss: 0.1228 - accuracy: 0.9576 - val_loss: 0.4161 - val_accuracy: 0.8499\n",
            "Epoch 12/30\n",
            "625/625 - 2s - loss: 0.1129 - accuracy: 0.9617 - val_loss: 0.4384 - val_accuracy: 0.8472\n",
            "Epoch 13/30\n",
            "625/625 - 2s - loss: 0.1020 - accuracy: 0.9656 - val_loss: 0.4703 - val_accuracy: 0.8414\n",
            "Epoch 14/30\n",
            "625/625 - 2s - loss: 0.0950 - accuracy: 0.9671 - val_loss: 0.4885 - val_accuracy: 0.8429\n",
            "Epoch 15/30\n",
            "625/625 - 2s - loss: 0.0872 - accuracy: 0.9712 - val_loss: 0.5102 - val_accuracy: 0.8442\n",
            "Epoch 16/30\n",
            "625/625 - 2s - loss: 0.0810 - accuracy: 0.9732 - val_loss: 0.5483 - val_accuracy: 0.8365\n",
            "Epoch 17/30\n",
            "625/625 - 2s - loss: 0.0752 - accuracy: 0.9757 - val_loss: 0.5640 - val_accuracy: 0.8374\n",
            "Epoch 18/30\n",
            "625/625 - 2s - loss: 0.0684 - accuracy: 0.9786 - val_loss: 0.6017 - val_accuracy: 0.8292\n",
            "Epoch 19/30\n",
            "625/625 - 2s - loss: 0.0640 - accuracy: 0.9791 - val_loss: 0.6232 - val_accuracy: 0.8307\n",
            "Epoch 20/30\n",
            "625/625 - 2s - loss: 0.0596 - accuracy: 0.9815 - val_loss: 0.6538 - val_accuracy: 0.8284\n",
            "Epoch 21/30\n",
            "625/625 - 2s - loss: 0.0534 - accuracy: 0.9834 - val_loss: 0.7375 - val_accuracy: 0.8244\n",
            "Epoch 22/30\n",
            "625/625 - 2s - loss: 0.0511 - accuracy: 0.9841 - val_loss: 0.7152 - val_accuracy: 0.8237\n",
            "Epoch 23/30\n",
            "625/625 - 2s - loss: 0.0481 - accuracy: 0.9857 - val_loss: 0.7740 - val_accuracy: 0.8244\n",
            "Epoch 24/30\n",
            "625/625 - 2s - loss: 0.0426 - accuracy: 0.9880 - val_loss: 0.7992 - val_accuracy: 0.8223\n",
            "Epoch 25/30\n",
            "625/625 - 2s - loss: 0.0404 - accuracy: 0.9881 - val_loss: 0.8350 - val_accuracy: 0.8195\n",
            "Epoch 26/30\n",
            "625/625 - 2s - loss: 0.0377 - accuracy: 0.9892 - val_loss: 0.8723 - val_accuracy: 0.8177\n",
            "Epoch 27/30\n",
            "625/625 - 2s - loss: 0.0344 - accuracy: 0.9902 - val_loss: 0.9382 - val_accuracy: 0.8150\n",
            "Epoch 28/30\n",
            "625/625 - 2s - loss: 0.0314 - accuracy: 0.9910 - val_loss: 1.0005 - val_accuracy: 0.8140\n",
            "Epoch 29/30\n",
            "625/625 - 2s - loss: 0.0306 - accuracy: 0.9911 - val_loss: 0.9537 - val_accuracy: 0.8159\n",
            "Epoch 30/30\n",
            "625/625 - 2s - loss: 0.0286 - accuracy: 0.9926 - val_loss: 1.0102 - val_accuracy: 0.8147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SigOHCBfUrYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810fce91-353e-4c21-81e5-b64dc223147b"
      },
      "source": [
        "#Testando\n",
        "sentence = [\"top snake handler leaves sinking huckabee campaign\"]\n",
        "sequences = TK.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9973521]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq9mSXB7FLtG"
      },
      "source": [
        "TESTES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TErmL-MfFqW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e11e98-2343-4c24-ec98-aad9a7f54b12"
      },
      "source": [
        "len(model.weights)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z1c8ZG0HsNo"
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DyY3UY0QNNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bae8dc2-90c2-4c61-b3e2-758eec6be482"
      },
      "source": [
        "layer = layers.Dense(3)\n",
        "layer.weights  # Empty"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0BXPVCFQPli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3d4cb8-ee34-4a99-8db4-da70d3a3bf0f"
      },
      "source": [
        "# Call layer on a test input\n",
        "x = tf.ones((1, 4))\n",
        "y = layer(x)\n",
        "layer.weights  # Now it has weights, of shape (4, 3) and (3,)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_2/kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
              " array([[ 0.6746961 ,  0.46956277,  0.84156907],\n",
              "        [ 0.27659857,  0.3058219 ,  0.46519685],\n",
              "        [ 0.18105757, -0.8942871 ,  0.56895053],\n",
              "        [-0.14560658, -0.24164319, -0.28311706]], dtype=float32)>,\n",
              " <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}